{"296c76e2-158b-4482-99e3-b14ef348bc65_1":{"average_precision_score_macro":[0.9967628205128205],"f1_score_micro":[0.9705882352941178],"balanced_accuracy":[0.9684722222222222],"norm_macro_recall":[0.9369444444444444],"AUC_macro":[0.9969444444444445],"precision_score_weighted":[0.9726747109100049],"recall_score_micro":[0.9705882352941178],"matthews_correlation":[0.9386123000100224],"precision_score_micro":[0.9705882352941178],"average_precision_score_micro":[0.9958391762277665],"accuracy":[0.9705882352941178],"log_loss":[0.06953465307505337],"weighted_accuracy":[0.9720097939196082],"recall_score_macro":[0.9684722222222222],"average_precision_score_weighted":[0.997325289089995],"precision_score_macro":[0.9704594017094017],"recall_score_weighted":[0.9705882352941178],"f1_score_macro":[0.9680371683587795],"AUC_micro":[0.9955017301038062],"f1_score_weighted":[0.970518587408613],"AUC_weighted":[0.9969444444444445]},"296c76e2-158b-4482-99e3-b14ef348bc65_3":{"average_precision_score_micro":[0.9974351969890124],"recall_score_macro":[0.9784722222222223],"matthews_correlation":[0.9526785151252903],"balanced_accuracy":[0.9784722222222223],"precision_score_weighted":[0.9781045751633988],"average_precision_score_weighted":[0.9987581699346405],"AUC_micro":[0.9972318339100348],"precision_score_micro":[0.9764705882352942],"log_loss":[0.283771576708101],"weighted_accuracy":[0.9749683738012651],"precision_score_macro":[0.9743055555555555],"f1_score_macro":[0.9755927239143352],"f1_score_micro":[0.9764705882352942],"accuracy":[0.9764705882352942],"recall_score_micro":[0.9764705882352942],"AUC_macro":[0.9986111111111111],"recall_score_weighted":[0.9764705882352942],"average_precision_score_macro":[0.99875],"norm_macro_recall":[0.9569444444444445],"f1_score_weighted":[0.9766100906765869],"AUC_weighted":[0.9986111111111111]},"296c76e2-158b-4482-99e3-b14ef348bc65_4":{"average_precision_score_micro":[0.996477594649831],"accuracy":[0.9705882352941178],"average_precision_score_macro":[0.99875],"precision_score_weighted":[0.9726747109100049],"AUC_weighted":[0.9986111111111111],"precision_score_macro":[0.9704594017094017],"f1_score_macro":[0.9680371683587795],"f1_score_micro":[0.9705882352941178],"balanced_accuracy":[0.9684722222222222],"AUC_micro":[0.9961937716262976],"average_precision_score_weighted":[0.9987581699346405],"AUC_macro":[0.9986111111111111],"recall_score_micro":[0.9705882352941178],"weighted_accuracy":[0.9720097939196082],"log_loss":[0.14099732417688277],"recall_score_macro":[0.9684722222222222],"norm_macro_recall":[0.9369444444444444],"recall_score_weighted":[0.9705882352941178],"precision_score_micro":[0.9705882352941178],"f1_score_weighted":[0.970518587408613],"matthews_correlation":[0.9386123000100224]},"296c76e2-158b-4482-99e3-b14ef348bc65_0":{"f1_score_macro":[0.9739603709731586],"norm_macro_recall":[0.9494444444444443],"recall_score_weighted":[0.9764705882352942],"f1_score_weighted":[0.9764009403497894],"average_precision_score_weighted":[0.997325289089995],"recall_score_macro":[0.9747222222222222],"matthews_correlation":[0.9511123000100223],"AUC_macro":[0.9969444444444445],"accuracy":[0.9764705882352942],"average_precision_score_micro":[0.9961467502377627],"f1_score_micro":[0.9764705882352942],"recall_score_micro":[0.9764705882352942],"balanced_accuracy":[0.9747222222222222],"average_precision_score_macro":[0.9967628205128205],"precision_score_macro":[0.9767094017094017],"AUC_weighted":[0.9969444444444445],"precision_score_weighted":[0.9792106586224232],"weighted_accuracy":[0.9775270352989185],"log_loss":[0.09169236604145156],"AUC_micro":[0.9958477508650517],"precision_score_micro":[0.9764705882352942]},"296c76e2-158b-4482-99e3-b14ef348bc65_15":{"recall_score_macro":[0.9747222222222222],"precision_score_weighted":[0.9792106586224232],"weighted_accuracy":[0.9775270352989185],"matthews_correlation":[0.9511123000100223],"log_loss":[0.07861864365502563],"norm_macro_recall":[0.9494444444444443],"AUC_micro":[0.9986159169550172],"AUC_weighted":[1.0],"f1_score_weighted":[0.9764009403497894],"accuracy":[0.9764705882352942],"recall_score_weighted":[0.9764705882352942],"precision_score_micro":[0.9764705882352942],"balanced_accuracy":[0.9747222222222222],"average_precision_score_weighted":[1.0],"average_precision_score_macro":[1.0],"f1_score_macro":[0.9739603709731586],"f1_score_micro":[0.9764705882352942],"recall_score_micro":[0.9764705882352942],"precision_score_macro":[0.9767094017094017],"AUC_macro":[1.0],"average_precision_score_micro":[0.9986928104575163]},"296c76e2-158b-4482-99e3-b14ef348bc65_18":{"balanced_accuracy":[0.9684722222222222],"accuracy":[0.9705882352941178],"f1_score_micro":[0.9705882352941178],"average_precision_score_micro":[0.996477594649831],"matthews_correlation":[0.9386123000100224],"AUC_macro":[0.9986111111111111],"average_precision_score_macro":[0.99875],"log_loss":[0.12424090632583987],"precision_score_micro":[0.9705882352941178],"recall_score_macro":[0.9684722222222222],"average_precision_score_weighted":[0.9987581699346405],"precision_score_weighted":[0.9726747109100049],"AUC_weighted":[0.9986111111111111],"weighted_accuracy":[0.9720097939196082],"AUC_micro":[0.9961937716262976],"precision_score_macro":[0.9704594017094017],"norm_macro_recall":[0.9369444444444444],"f1_score_macro":[0.9680371683587795],"recall_score_weighted":[0.9705882352941178],"f1_score_weighted":[0.970518587408613],"recall_score_micro":[0.9705882352941178]},"296c76e2-158b-4482-99e3-b14ef348bc65_10":{"average_precision_score_macro":[0.99875],"recall_score_macro":[0.9684722222222222],"precision_score_micro":[0.9705882352941178],"f1_score_macro":[0.9680371683587795],"AUC_macro":[0.9986111111111111],"recall_score_weighted":[0.9705882352941178],"average_precision_score_weighted":[0.9987581699346405],"balanced_accuracy":[0.9684722222222222],"average_precision_score_micro":[0.9971083996033914],"f1_score_micro":[0.9705882352941178],"log_loss":[0.14233186299666456],"matthews_correlation":[0.9386123000100224],"AUC_weighted":[0.9986111111111111],"accuracy":[0.9705882352941178],"f1_score_weighted":[0.970518587408613],"precision_score_weighted":[0.9726747109100049],"precision_score_macro":[0.9704594017094017],"norm_macro_recall":[0.9369444444444444],"weighted_accuracy":[0.9720097939196082],"AUC_micro":[0.996885813148789],"recall_score_micro":[0.9705882352941178]},"296c76e2-158b-4482-99e3-b14ef348bc65_12":{"norm_macro_recall":[0.9494444444444443],"precision_score_micro":[0.9764705882352942],"recall_score_weighted":[0.9764705882352942],"f1_score_weighted":[0.9764009403497894],"f1_score_macro":[0.9739603709731586],"balanced_accuracy":[0.9747222222222222],"accuracy":[0.9764705882352942],"AUC_weighted":[1.0],"precision_score_weighted":[0.9792106586224232],"recall_score_micro":[0.9764705882352942],"matthews_correlation":[0.9511123000100223],"average_precision_score_macro":[1.0],"AUC_macro":[1.0],"f1_score_micro":[0.9764705882352942],"recall_score_macro":[0.9747222222222222],"average_precision_score_weighted":[1.0],"log_loss":[0.08276036113345764],"weighted_accuracy":[0.9775270352989185],"average_precision_score_micro":[0.9986928104575163],"precision_score_macro":[0.9767094017094017],"AUC_micro":[0.9986159169550172]},"296c76e2-158b-4482-99e3-b14ef348bc65_13":{"recall_score_weighted":[0.9764705882352942],"recall_score_macro":[0.9747222222222222],"log_loss":[0.041735018418679284],"AUC_weighted":[1.0],"precision_score_macro":[0.9767094017094017],"AUC_micro":[0.9986159169550172],"weighted_accuracy":[0.9775270352989185],"f1_score_micro":[0.9764705882352942],"precision_score_weighted":[0.9792106586224232],"average_precision_score_macro":[1.0],"accuracy":[0.9764705882352942],"recall_score_micro":[0.9764705882352942],"f1_score_macro":[0.9739603709731586],"norm_macro_recall":[0.9494444444444443],"average_precision_score_weighted":[1.0],"matthews_correlation":[0.9511123000100223],"balanced_accuracy":[0.9747222222222222],"AUC_macro":[1.0],"average_precision_score_micro":[0.9986928104575163],"f1_score_weighted":[0.9764009403497894],"precision_score_micro":[0.9764705882352942]},"296c76e2-158b-4482-99e3-b14ef348bc65_14":{"f1_score_macro":[0.9443434979081433],"weighted_accuracy":[0.9485615180575392],"accuracy":[0.9470588235294117],"recall_score_weighted":[0.9470588235294117],"AUC_macro":[0.9944444444444442],"AUC_micro":[0.9913494809688583],"f1_score_weighted":[0.9467844972813341],"matthews_correlation":[0.8933996344249813],"average_precision_score_weighted":[0.9951767676767677],"precision_score_micro":[0.9470588235294117],"f1_score_micro":[0.9470588235294117],"average_precision_score_macro":[0.9951781705948374],"AUC_weighted":[0.9944444444444445],"precision_score_weighted":[0.9509286791639733],"norm_macro_recall":[0.8897222222222222],"recall_score_micro":[0.9470588235294117],"average_precision_score_micro":[0.9922659012005214],"recall_score_macro":[0.9448611111111112],"log_loss":[0.32475337134579546],"precision_score_macro":[0.9488721001221002],"balanced_accuracy":[0.9448611111111112]},"296c76e2-158b-4482-99e3-b14ef348bc65_19":{"accuracy":[0.9529411764705884],"precision_score_macro":[0.9591841491841493],"norm_macro_recall":[0.9036111111111111],"precision_score_weighted":[0.962128068010421],"recall_score_macro":[0.9518055555555556],"precision_score_micro":[0.9529411764705884],"AUC_macro":[1.0],"f1_score_macro":[0.950059872177149],"matthews_correlation":[0.9105878451356796],"AUC_micro":[0.9903114186851212],"AUC_weighted":[1.0],"average_precision_score_weighted":[1.0],"average_precision_score_micro":[0.9912658050836434],"recall_score_weighted":[0.9529411764705884],"f1_score_micro":[0.9529411764705884],"log_loss":[0.14987252274477245],"recall_score_micro":[0.9529411764705884],"f1_score_weighted":[0.9523770325410519],"balanced_accuracy":[0.9518055555555556],"weighted_accuracy":[0.9533891042644358],"average_precision_score_macro":[1.0]},"296c76e2-158b-4482-99e3-b14ef348bc65_21":{"matthews_correlation":[0.928604474895795],"recall_score_micro":[0.9647058823529413],"AUC_micro":[0.9965397923875432],"f1_score_macro":[0.9621139657444007],"precision_score_micro":[0.9647058823529413],"balanced_accuracy":[0.9629166666666666],"weighted_accuracy":[0.965802897367884],"log_loss":[0.10119714890899575],"average_precision_score_macro":[0.99875],"norm_macro_recall":[0.9258333333333333],"precision_score_weighted":[0.9684917043740573],"f1_score_micro":[0.9647058823529413],"recall_score_weighted":[0.9647058823529413],"f1_score_weighted":[0.9645545351210313],"AUC_macro":[0.9986111111111111],"recall_score_macro":[0.9629166666666666],"average_precision_score_micro":[0.9968008255933952],"AUC_weighted":[0.9986111111111111],"precision_score_macro":[0.9660149572649572],"accuracy":[0.9647058823529413],"average_precision_score_weighted":[0.9987581699346405]},"296c76e2-158b-4482-99e3-b14ef348bc65_26":{"precision_score_micro":[0.9705882352941178],"recall_score_micro":[0.9705882352941178],"accuracy":[0.9705882352941178],"precision_score_macro":[0.9704594017094017],"average_precision_score_weighted":[0.997516339869281],"balanced_accuracy":[0.9684722222222222],"recall_score_macro":[0.9684722222222222],"AUC_micro":[0.9955017301038062],"f1_score_macro":[0.9680371683587795],"f1_score_weighted":[0.970518587408613],"norm_macro_recall":[0.9369444444444444],"f1_score_micro":[0.9705882352941178],"precision_score_weighted":[0.9726747109100049],"average_precision_score_macro":[0.9975000000000002],"average_precision_score_micro":[0.9958473425489908],"log_loss":[0.13775454109486338],"weighted_accuracy":[0.9720097939196082],"AUC_weighted":[0.9972222222222221],"recall_score_weighted":[0.9705882352941178],"matthews_correlation":[0.9386123000100224],"AUC_macro":[0.9972222222222221]},"296c76e2-158b-4482-99e3-b14ef348bc65_22":{"accuracy":[0.9529411764705884],"matthews_correlation":[0.9080956702499069],"norm_macro_recall":[0.9022222222222223],"precision_score_weighted":[0.9597751268339504],"f1_score_macro":[0.950059872177149],"recall_score_macro":[0.9511111111111111],"average_precision_score_macro":[0.9952553310886645],"weighted_accuracy":[0.9540787594368496],"recall_score_weighted":[0.9529411764705884],"balanced_accuracy":[0.9511111111111111],"log_loss":[0.29889844694038553],"average_precision_score_micro":[0.9926045614324162],"recall_score_micro":[0.9529411764705884],"AUC_weighted":[0.9944444444444445],"average_precision_score_weighted":[0.995258467023173],"f1_score_weighted":[0.9524587318874571],"f1_score_micro":[0.9529411764705884],"AUC_macro":[0.9944444444444445],"precision_score_macro":[0.9573785936285937],"precision_score_micro":[0.9529411764705884],"AUC_micro":[0.9916955017301037]},"296c76e2-158b-4482-99e3-b14ef348bc65_23":{"recall_score_micro":[0.9764705882352942],"matthews_correlation":[0.9511123000100223],"weighted_accuracy":[0.9775270352989185],"average_precision_score_weighted":[1.0],"precision_score_micro":[0.9764705882352942],"precision_score_weighted":[0.9792106586224232],"f1_score_micro":[0.9764705882352942],"recall_score_macro":[0.9747222222222222],"accuracy":[0.9764705882352942],"f1_score_weighted":[0.9764009403497894],"average_precision_score_micro":[0.9986928104575163],"AUC_macro":[1.0],"AUC_micro":[0.9986159169550172],"recall_score_weighted":[0.9764705882352942],"balanced_accuracy":[0.9747222222222222],"log_loss":[0.07451981468445137],"precision_score_macro":[0.9767094017094017],"norm_macro_recall":[0.9494444444444443],"average_precision_score_macro":[1.0],"f1_score_macro":[0.9739603709731586],"AUC_weighted":[1.0]},"296c76e2-158b-4482-99e3-b14ef348bc65_31":{"average_precision_score_macro":[1.0],"precision_score_weighted":[0.9792106586224232],"AUC_weighted":[1.0],"average_precision_score_micro":[0.9986928104575163],"norm_macro_recall":[0.9494444444444443],"accuracy":[0.9764705882352942],"matthews_correlation":[0.9511123000100223],"AUC_micro":[0.9986159169550172],"average_precision_score_weighted":[1.0],"f1_score_macro":[0.9739603709731586],"precision_score_macro":[0.9767094017094017],"f1_score_weighted":[0.9764009403497894],"AUC_macro":[1.0],"weighted_accuracy":[0.9775270352989185],"f1_score_micro":[0.9764705882352942],"precision_score_micro":[0.9764705882352942],"recall_score_macro":[0.9747222222222222],"balanced_accuracy":[0.9747222222222222],"log_loss":[0.05248922787307249],"recall_score_micro":[0.9764705882352942],"recall_score_weighted":[0.9764705882352942]},"296c76e2-158b-4482-99e3-b14ef348bc65_28":{"recall_score_micro":[0.9764705882352942],"precision_score_weighted":[0.9792106586224232],"average_precision_score_micro":[0.9980543920354519],"recall_score_macro":[0.9747222222222222],"balanced_accuracy":[0.9747222222222222],"recall_score_weighted":[0.9764705882352942],"AUC_macro":[1.0],"f1_score_weighted":[0.9764009403497894],"matthews_correlation":[0.9511123000100223],"f1_score_micro":[0.9764705882352942],"precision_score_macro":[0.9767094017094017],"log_loss":[0.08434141977308254],"weighted_accuracy":[0.9775270352989185],"f1_score_macro":[0.9739603709731586],"AUC_micro":[0.9979238754325259],"precision_score_micro":[0.9764705882352942],"AUC_weighted":[1.0],"accuracy":[0.9764705882352942],"average_precision_score_weighted":[1.0],"average_precision_score_macro":[1.0],"norm_macro_recall":[0.9494444444444443]},"296c76e2-158b-4482-99e3-b14ef348bc65_32":{"AUC_micro":[0.9986159169550172],"norm_macro_recall":[0.9494444444444443],"precision_score_weighted":[0.9792106586224232],"recall_score_macro":[0.9747222222222222],"AUC_weighted":[1.0],"average_precision_score_weighted":[1.0],"log_loss":[0.08485927466345136],"accuracy":[0.9764705882352942],"average_precision_score_micro":[0.9986928104575163],"precision_score_macro":[0.9767094017094017],"AUC_macro":[1.0],"f1_score_weighted":[0.9764009403497894],"f1_score_macro":[0.9739603709731586],"recall_score_micro":[0.9764705882352942],"recall_score_weighted":[0.9764705882352942],"weighted_accuracy":[0.9775270352989185],"balanced_accuracy":[0.9747222222222222],"average_precision_score_macro":[1.0],"precision_score_micro":[0.9764705882352942],"f1_score_micro":[0.9764705882352942],"matthews_correlation":[0.9511123000100223]},"296c76e2-158b-4482-99e3-b14ef348bc65_25":{"f1_score_weighted":[0.9822832932909659],"average_precision_score_macro":[1.0],"balanced_accuracy":[0.9802777777777777],"norm_macro_recall":[0.9605555555555554],"recall_score_macro":[0.9802777777777777],"AUC_weighted":[1.0],"recall_score_weighted":[0.9823529411764707],"average_precision_score_weighted":[1.0],"weighted_accuracy":[0.9837339318506426],"accuracy":[0.9823529411764707],"f1_score_micro":[0.9823529411764707],"AUC_macro":[1.0],"precision_score_macro":[0.9822649572649572],"average_precision_score_micro":[0.9990196078431373],"log_loss":[0.20850938567433724],"precision_score_weighted":[0.9844394167923578],"recall_score_micro":[0.9823529411764707],"matthews_correlation":[0.9622234111211334],"f1_score_macro":[0.979842723914335],"AUC_micro":[0.998961937716263],"precision_score_micro":[0.9823529411764707]},"296c76e2-158b-4482-99e3-b14ef348bc65_17":{"AUC_weighted":[1.0],"AUC_macro":[1.0],"average_precision_score_micro":[0.9990196078431373],"matthews_correlation":[0.9622234111211334],"AUC_micro":[0.998961937716263],"precision_score_micro":[0.9823529411764707],"weighted_accuracy":[0.9837339318506426],"recall_score_weighted":[0.9823529411764707],"recall_score_macro":[0.9802777777777777],"f1_score_micro":[0.9823529411764707],"balanced_accuracy":[0.9802777777777777],"norm_macro_recall":[0.9605555555555554],"recall_score_micro":[0.9823529411764707],"precision_score_weighted":[0.9844394167923578],"f1_score_macro":[0.979842723914335],"average_precision_score_weighted":[1.0],"f1_score_weighted":[0.9822832932909659],"accuracy":[0.9823529411764707],"average_precision_score_macro":[1.0],"precision_score_macro":[0.9822649572649572],"log_loss":[0.06804900017192188]},"296c76e2-158b-4482-99e3-b14ef348bc65_29":{"average_precision_score_micro":[0.9986928104575163],"accuracy":[0.9764705882352942],"f1_score_micro":[0.9764705882352942],"average_precision_score_weighted":[1.0],"recall_score_weighted":[0.9764705882352942],"recall_score_micro":[0.9764705882352942],"AUC_weighted":[1.0],"f1_score_macro":[0.9739603709731586],"norm_macro_recall":[0.9494444444444443],"precision_score_macro":[0.9767094017094017],"f1_score_weighted":[0.9764009403497894],"AUC_micro":[0.9986159169550172],"precision_score_weighted":[0.9792106586224232],"average_precision_score_macro":[1.0],"recall_score_macro":[0.9747222222222222],"AUC_macro":[1.0],"weighted_accuracy":[0.9775270352989185],"log_loss":[0.04285313474260509],"balanced_accuracy":[0.9747222222222222],"precision_score_micro":[0.9764705882352942],"matthews_correlation":[0.9511123000100223]},"296c76e2-158b-4482-99e3-b14ef348bc65_34":{"precision_score_micro":[0.9823529411764707],"recall_score_micro":[0.9823529411764707],"recall_score_weighted":[0.9823529411764707],"matthews_correlation":[0.9622234111211334],"f1_score_macro":[0.979842723914335],"norm_macro_recall":[0.9605555555555554],"AUC_weighted":[1.0],"f1_score_micro":[0.9823529411764707],"AUC_macro":[1.0],"AUC_micro":[0.998961937716263],"log_loss":[0.10256983559515873],"average_precision_score_weighted":[1.0],"balanced_accuracy":[0.9802777777777777],"accuracy":[0.9823529411764707],"average_precision_score_macro":[1.0],"weighted_accuracy":[0.9837339318506426],"f1_score_weighted":[0.9822832932909659],"precision_score_weighted":[0.9844394167923578],"recall_score_macro":[0.9802777777777777],"precision_score_macro":[0.9822649572649572],"average_precision_score_micro":[0.9990196078431373]},"296c76e2-158b-4482-99e3-b14ef348bc65_38":{"log_loss":[0.10470023596866598],"f1_score_micro":[0.9823529411764707],"average_precision_score_weighted":[1.0],"accuracy":[0.9823529411764707],"average_precision_score_micro":[0.9990196078431373],"recall_score_macro":[0.9802777777777777],"balanced_accuracy":[0.9802777777777777],"average_precision_score_macro":[1.0],"weighted_accuracy":[0.9837339318506426],"recall_score_weighted":[0.9823529411764707],"matthews_correlation":[0.9622234111211334],"AUC_macro":[1.0],"AUC_weighted":[1.0],"precision_score_macro":[0.9822649572649572],"f1_score_weighted":[0.9822832932909659],"precision_score_weighted":[0.9844394167923578],"AUC_micro":[0.998961937716263],"f1_score_macro":[0.979842723914335],"precision_score_micro":[0.9823529411764707],"norm_macro_recall":[0.9605555555555554],"recall_score_micro":[0.9823529411764707]},"296c76e2-158b-4482-99e3-b14ef348bc65_5":{"precision_score_micro":[0.9647058823529413],"matthews_correlation":[0.9297577003309744],"recall_score_micro":[0.9647058823529413],"AUC_macro":[0.9944444444444445],"AUC_micro":[0.9930795847750865],"average_precision_score_weighted":[0.9951767676767677],"accuracy":[0.9647058823529413],"recall_score_weighted":[0.9647058823529413],"average_precision_score_micro":[0.9938654884038238],"f1_score_micro":[0.9647058823529413],"precision_score_macro":[0.9684554334554335],"precision_score_weighted":[0.9676607053077643],"AUC_weighted":[0.9944444444444445],"f1_score_weighted":[0.9643333333333335],"f1_score_macro":[0.9629509803921568],"norm_macro_recall":[0.9230555555555556],"recall_score_macro":[0.9615277777777779],"log_loss":[0.3625386927838666],"balanced_accuracy":[0.9615277777777779],"average_precision_score_macro":[0.9951781705948374],"weighted_accuracy":[0.9666965925321364]},"296c76e2-158b-4482-99e3-b14ef348bc65_9":{"AUC_macro":[0.9972222222222221],"f1_score_macro":[0.9553945981554678],"average_precision_score_micro":[0.9946133788912883],"AUC_weighted":[0.9972222222222221],"balanced_accuracy":[0.95875],"recall_score_macro":[0.95875],"f1_score_weighted":[0.9588116846211475],"matthews_correlation":[0.9160052122433078],"recall_score_weighted":[0.9588235294117646],"average_precision_score_macro":[0.9975000000000002],"weighted_accuracy":[0.9587023056519077],"precision_score_weighted":[0.9635897435897436],"accuracy":[0.9588235294117646],"average_precision_score_weighted":[0.997516339869281],"precision_score_micro":[0.9588235294117646],"precision_score_macro":[0.9576816239316239],"AUC_micro":[0.9941176470588236],"recall_score_micro":[0.9588235294117646],"log_loss":[0.3024174996529746],"norm_macro_recall":[0.9175000000000001],"f1_score_micro":[0.9588235294117646]},"296c76e2-158b-4482-99e3-b14ef348bc65_2":{"f1_score_weighted":[0.9764009403497894],"f1_score_micro":[0.9764705882352942],"precision_score_weighted":[0.9792106586224232],"AUC_micro":[0.9972318339100346],"f1_score_macro":[0.9739603709731586],"recall_score_micro":[0.9764705882352942],"matthews_correlation":[0.9511123000100223],"AUC_weighted":[1.0],"precision_score_macro":[0.9767094017094017],"precision_score_micro":[0.9764705882352942],"average_precision_score_micro":[0.9974235870818916],"weighted_accuracy":[0.9775270352989185],"balanced_accuracy":[0.9747222222222222],"accuracy":[0.9764705882352942],"AUC_macro":[1.0],"average_precision_score_weighted":[1.0],"norm_macro_recall":[0.9494444444444443],"average_precision_score_macro":[1.0],"log_loss":[0.09455074957567514],"recall_score_weighted":[0.9764705882352942],"recall_score_macro":[0.9747222222222222]},"296c76e2-158b-4482-99e3-b14ef348bc65_8":{"AUC_weighted":[0.9949999999999999],"f1_score_weighted":[0.9764009403497894],"weighted_accuracy":[0.9775270352989185],"average_precision_score_micro":[0.9961567166973742],"average_precision_score_weighted":[0.9951960784313727],"recall_score_weighted":[0.9764705882352942],"average_precision_score_macro":[0.993],"recall_score_micro":[0.9764705882352942],"balanced_accuracy":[0.9747222222222222],"f1_score_macro":[0.9739603709731586],"accuracy":[0.9764705882352942],"f1_score_micro":[0.9764705882352942],"log_loss":[0.07873469696403061],"matthews_correlation":[0.9511123000100223],"precision_score_weighted":[0.9792106586224232],"recall_score_macro":[0.9747222222222222],"precision_score_micro":[0.9764705882352942],"AUC_micro":[0.995847750865052],"AUC_macro":[0.9949999999999999],"norm_macro_recall":[0.9494444444444443],"precision_score_macro":[0.9767094017094017]},"296c76e2-158b-4482-99e3-b14ef348bc65_7":{"recall_score_weighted":[0.9764705882352942],"f1_score_micro":[0.9764705882352942],"accuracy":[0.9764705882352942],"average_precision_score_weighted":[1.0],"f1_score_weighted":[0.9764009403497894],"matthews_correlation":[0.9511123000100223],"average_precision_score_macro":[1.0],"precision_score_weighted":[0.9792106586224232],"AUC_macro":[1.0],"precision_score_macro":[0.9767094017094017],"balanced_accuracy":[0.9747222222222222],"AUC_weighted":[1.0],"log_loss":[0.048804713895068776],"average_precision_score_micro":[0.9986928104575163],"recall_score_micro":[0.9764705882352942],"norm_macro_recall":[0.9494444444444443],"recall_score_macro":[0.9747222222222222],"precision_score_micro":[0.9764705882352942],"AUC_micro":[0.9986159169550172],"f1_score_macro":[0.9739603709731586],"weighted_accuracy":[0.9775270352989185]},"296c76e2-158b-4482-99e3-b14ef348bc65_6":{"AUC_macro":[0.9969444444444445],"matthews_correlation":[0.9386123000100224],"precision_score_weighted":[0.9726747109100049],"f1_score_macro":[0.9680371683587795],"average_precision_score_macro":[0.9967628205128205],"precision_score_micro":[0.9705882352941178],"recall_score_weighted":[0.9705882352941178],"average_precision_score_micro":[0.9971083996033914],"f1_score_weighted":[0.970518587408613],"recall_score_macro":[0.9684722222222222],"f1_score_micro":[0.9705882352941178],"log_loss":[0.2590568021043464],"AUC_weighted":[0.9969444444444445],"recall_score_micro":[0.9705882352941178],"weighted_accuracy":[0.9720097939196082],"balanced_accuracy":[0.9684722222222222],"average_precision_score_weighted":[0.997325289089995],"norm_macro_recall":[0.9369444444444444],"accuracy":[0.9705882352941178],"precision_score_macro":[0.9704594017094017],"AUC_micro":[0.996885813148789]},"296c76e2-158b-4482-99e3-b14ef348bc65_11":{"precision_score_weighted":[0.9726747109100049],"recall_score_micro":[0.9705882352941178],"precision_score_macro":[0.9704594017094017],"log_loss":[0.27928275608402325],"matthews_correlation":[0.9386123000100224],"recall_score_macro":[0.9684722222222222],"accuracy":[0.9705882352941178],"average_precision_score_weighted":[0.9987581699346405],"weighted_accuracy":[0.9720097939196082],"AUC_macro":[0.9986111111111111],"precision_score_micro":[0.9705882352941178],"f1_score_micro":[0.9705882352941178],"AUC_weighted":[0.9986111111111111],"average_precision_score_micro":[0.9977468180254558],"average_precision_score_macro":[0.99875],"balanced_accuracy":[0.9684722222222222],"f1_score_weighted":[0.970518587408613],"AUC_micro":[0.9975778546712804],"norm_macro_recall":[0.9369444444444444],"recall_score_weighted":[0.9705882352941178],"f1_score_macro":[0.9680371683587795]},"296c76e2-158b-4482-99e3-b14ef348bc65_16":{"f1_score_micro":[0.9764705882352942],"recall_score_micro":[0.9764705882352942],"matthews_correlation":[0.9511123000100223],"f1_score_macro":[0.9739603709731586],"average_precision_score_weighted":[1.0],"recall_score_macro":[0.9747222222222222],"precision_score_macro":[0.9767094017094017],"recall_score_weighted":[0.9764705882352942],"accuracy":[0.9764705882352942],"AUC_micro":[0.9986159169550172],"AUC_weighted":[1.0],"precision_score_weighted":[0.9792106586224232],"average_precision_score_macro":[1.0],"f1_score_weighted":[0.9764009403497894],"norm_macro_recall":[0.9494444444444443],"average_precision_score_micro":[0.9986928104575163],"AUC_macro":[1.0],"weighted_accuracy":[0.9775270352989185],"precision_score_micro":[0.9764705882352942],"balanced_accuracy":[0.9747222222222222],"log_loss":[0.061556207339617766]},"296c76e2-158b-4482-99e3-b14ef348bc65_33":{"f1_score_micro":[0.9764705882352942],"average_precision_score_micro":[0.9986928104575163],"matthews_correlation":[0.9511123000100223],"average_precision_score_macro":[1.0],"recall_score_micro":[0.9764705882352942],"precision_score_macro":[0.9767094017094017],"log_loss":[0.05428317131764757],"recall_score_macro":[0.9747222222222222],"norm_macro_recall":[0.9494444444444443],"f1_score_weighted":[0.9764009403497894],"AUC_micro":[0.9986159169550172],"f1_score_macro":[0.9739603709731586],"AUC_macro":[1.0],"weighted_accuracy":[0.9775270352989185],"balanced_accuracy":[0.9747222222222222],"recall_score_weighted":[0.9764705882352942],"accuracy":[0.9764705882352942],"average_precision_score_weighted":[1.0],"precision_score_micro":[0.9764705882352942],"precision_score_weighted":[0.9792106586224232],"AUC_weighted":[1.0]},"296c76e2-158b-4482-99e3-b14ef348bc65_24":{"precision_score_micro":[0.9823529411764707],"norm_macro_recall":[0.9605555555555554],"f1_score_micro":[0.9823529411764707],"AUC_macro":[0.9966666666666667],"recall_score_micro":[0.9823529411764707],"recall_score_weighted":[0.9823529411764707],"weighted_accuracy":[0.9837339318506426],"average_precision_score_weighted":[0.9973766429648784],"precision_score_weighted":[0.9844394167923578],"matthews_correlation":[0.9622234111211334],"f1_score_weighted":[0.9822832932909659],"average_precision_score_macro":[0.996475122100122],"f1_score_macro":[0.979842723914335],"balanced_accuracy":[0.9802777777777777],"log_loss":[0.09697331347511076],"precision_score_macro":[0.9822649572649572],"recall_score_macro":[0.9802777777777777],"AUC_micro":[0.9968858131487888],"average_precision_score_micro":[0.997111966045448],"AUC_weighted":[0.9966666666666667],"accuracy":[0.9823529411764707]},"296c76e2-158b-4482-99e3-b14ef348bc65_20":{"recall_score_weighted":[0.9352941176470588],"average_precision_score_macro":[1.0],"AUC_weighted":[1.0],"AUC_micro":[0.9837370242214533],"f1_score_micro":[0.9352941176470588],"recall_score_macro":[0.9349999999999999],"average_precision_score_micro":[0.9852994134774894],"f1_score_macro":[0.9315668872638856],"matthews_correlation":[0.879046503881599],"log_loss":[0.17850882380610023],"accuracy":[0.9352941176470588],"precision_score_macro":[0.9447018259518261],"average_precision_score_weighted":[1.0],"balanced_accuracy":[0.9349999999999999],"precision_score_micro":[0.9352941176470588],"weighted_accuracy":[0.9349535569374282],"f1_score_weighted":[0.9340052452296362],"norm_macro_recall":[0.8699999999999999],"recall_score_micro":[0.9352941176470588],"AUC_macro":[1.0],"precision_score_weighted":[0.9487516568398922]},"296c76e2-158b-4482-99e3-b14ef348bc65_30":{"AUC_macro":[0.9966666666666667],"f1_score_weighted":[0.9763596606800269],"f1_score_micro":[0.9764705882352942],"accuracy":[0.9764705882352942],"average_precision_score_weighted":[0.9973766429648784],"matthews_correlation":[0.9509646078676276],"weighted_accuracy":[0.9782166904713323],"AUC_micro":[0.9972318339100346],"f1_score_macro":[0.9738778116336334],"average_precision_score_micro":[0.9974235870818916],"recall_score_weighted":[0.9764705882352942],"balanced_accuracy":[0.9740277777777777],"average_precision_score_macro":[0.996475122100122],"precision_score_weighted":[0.9791452991452992],"precision_score_macro":[0.9772649572649573],"AUC_weighted":[0.9966666666666667],"precision_score_micro":[0.9764705882352942],"norm_macro_recall":[0.9480555555555554],"recall_score_macro":[0.9740277777777777],"recall_score_micro":[0.9764705882352942],"log_loss":[0.14435074357818634]},"296c76e2-158b-4482-99e3-b14ef348bc65_39":{"average_precision_score_macro":[1.0],"recall_score_weighted":[0.9823529411764707],"recall_score_micro":[0.9823529411764707],"precision_score_macro":[0.9822649572649572],"AUC_macro":[1.0],"accuracy":[0.9823529411764707],"f1_score_macro":[0.979842723914335],"balanced_accuracy":[0.9802777777777777],"f1_score_weighted":[0.9822832932909659],"precision_score_micro":[0.9823529411764707],"average_precision_score_weighted":[1.0],"norm_macro_recall":[0.9605555555555554],"f1_score_micro":[0.9823529411764707],"recall_score_macro":[0.9802777777777777],"precision_score_weighted":[0.9844394167923578],"AUC_weighted":[1.0],"average_precision_score_micro":[0.9983811894210728],"weighted_accuracy":[0.9837339318506426],"log_loss":[0.2339590306444852],"AUC_micro":[0.9982698961937716],"matthews_correlation":[0.9622234111211334]},"296c76e2-158b-4482-99e3-b14ef348bc65_27":{"precision_score_macro":[0.9822649572649572],"average_precision_score_micro":[0.9983811894210728],"AUC_macro":[0.9983333333333334],"AUC_weighted":[0.9983333333333334],"average_precision_score_weighted":[0.9985671191553545],"balanced_accuracy":[0.9802777777777777],"accuracy":[0.9823529411764707],"weighted_accuracy":[0.9837339318506426],"matthews_correlation":[0.9622234111211334],"precision_score_weighted":[0.9844394167923578],"AUC_micro":[0.9982698961937716],"precision_score_micro":[0.9823529411764707],"average_precision_score_macro":[0.9980128205128205],"recall_score_micro":[0.9823529411764707],"recall_score_weighted":[0.9823529411764707],"recall_score_macro":[0.9802777777777777],"f1_score_micro":[0.9823529411764707],"log_loss":[0.06470877571022829],"f1_score_macro":[0.979842723914335],"f1_score_weighted":[0.9822832932909659],"norm_macro_recall":[0.9605555555555554]}}